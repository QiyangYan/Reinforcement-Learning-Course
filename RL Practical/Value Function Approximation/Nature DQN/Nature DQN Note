Key Point: Fixed Targets 固定住目标Q网络
Nature DQN使用了两个Q网络，一个当前Q网络  用来选择动作，更新模型参数，另一个目标Q网络用于计算目标Q值。
目标Q网络的网络参数不需要迭代更新，而是每隔一段时间从当前Q网络复制过来，即延时更新。
要注意的是，两个Q网络的结构是一模一样的。这样才可以复制网络参数。

Function:
1. 减少目标Q值和当前的Q值相关性
2. Avoid an oscillated training history
因为这里的目标Q网络相当于老鼠，当前Q网络相当于猫，我们的目标是让猫追上老鼠，也就是当前Q网络逼近于目标Q网络。
那么，显然，老鼠不应该来回乱跑，否则，猫的优化轨迹很乱，使得当前Q网络无法得到较好的训练。

Network: 两个一模一样结构的Q网络
Comparison with regular DQN: 除了用一个新的相同结构的目标Q网络来计算目标Q值以外，其余部分基本是完全相同的。